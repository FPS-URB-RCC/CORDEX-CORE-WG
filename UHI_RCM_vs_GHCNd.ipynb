{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df9ec30-6c2f-46b8-895d-0b9c3c7ddae8",
   "metadata": {},
   "source": [
    "# UHI in RCMs and GHCNd station data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ded07b",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239a4b8-882b-43e9-b127-8c0d52198772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "#from pyesgf.logon import LogonManager\n",
    "#from pyesgf.search import SearchConnection\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2ce1b-4fa3-4d08-89c0-693a6605fa48",
   "metadata": {},
   "source": [
    "comments:\n",
    "1) RegCM4-0 no opendap access for EAS-22\n",
    "2) Beijing and Tokyo no available data GHCNd (data from Jiacan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee448972",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Globals and functions\n",
    "The following commodity functions and globals could go to a separate script to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1e088-b3cd-4113-b2bf-8acfeb875781",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/lustre/gmeteo/WORK/DATA/CORDEX-FPS-URB-RCC/nextcloud/'\n",
    "dest = '/lustre/gmeteo/WORK/diezsj/research/cordex-fps-urb-rcc/results/'\n",
    "ucdb_info = gpd.read_file(root  + 'CORDEX-CORE-WG/GHS_FUA_UCD/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_2.gpkg')\n",
    "\n",
    "rcms = {\n",
    "  'AFR-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'AUS-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'CAM-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'SAM-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'WAS-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'EUR-11': [\n",
    "    'REMO2015',\n",
    "    'RegCM4-6',\n",
    "  ],\n",
    "  'EAS-22': [\n",
    "#    'RegCM4-0', # No opendap access\n",
    "    'REMO2015'\n",
    "  ],\n",
    "  'SEA-22' : [\n",
    "    'REMO2015',\n",
    "    'RegCM4-7',\n",
    "  ],\n",
    "  'NAM-22': [\n",
    "    'RegCM4_v4-4-rc8',\n",
    "    'REMO2015',\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cec4d0-7833-45bb-8798-748ae1e66dde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ESGF related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6d7e6-62ab-430c-9a46-ec358c208bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"ESGF_PYCLIENT_NO_FACETS_STAR_WARNING\"] = \"1\"\n",
    "os.environ[\"MESA_LOADER_DRIVER_OVERRIDE\"]=\"i965\"\n",
    "# Deal with non-standard names...\n",
    "rlat_names = {'rlat', 'y'}\n",
    "rlon_names = {'rlon', 'x'}\n",
    "rotated_pole_names = {'rotated_pole', 'rotated_latitude_longitude', 'Lambert_Conformal',\n",
    "                     'oblique_mercator', 'crs'}\n",
    "\n",
    "def search_dic(var, dom, rcm):\n",
    "  return(dict(\n",
    "    project='CORDEX',\n",
    "    experiment=['evaluation',],\n",
    "    rcm_name = rcm,\n",
    "    domain = dom,\n",
    "    variable=[var,],\n",
    "    time_frequency = 'day',\n",
    "    facets = 'dataset_id'\n",
    "  ))\n",
    "\n",
    "def select_name(names, avail_names):\n",
    "  # Select variable/coordinate names among a list of potential names.\n",
    "  # Potential names are matched against those available in the data set.\n",
    "  return(list(names.intersection(list(avail_names)))[0])\n",
    "    \n",
    "def ESGF_login():\n",
    "  lm = LogonManager()\n",
    "  if not lm.is_logged_on():\n",
    "    with open(\"openid.json\") as fp:\n",
    "      lm.logon_with_openid(**json.load(fp))\n",
    "  if not lm.is_logged_on():\n",
    "    print(\"/!\\ There was some problem logging in\")\n",
    "\n",
    "def get_opendap_urls(vardic, ires=0):\n",
    "  nodeURL = 'http://esgf-data.dkrz.de/esg-search'\n",
    "  conn = SearchConnection(nodeURL, distrib=True)\n",
    "  ctx = conn.new_context(**vardic)\n",
    "  results = ctx.search(batch_size=200)\n",
    "  dids = [result.dataset_id for result in results]\n",
    "  print(dids)\n",
    "  files = results[ires].file_context().search()\n",
    "  return([file.opendap_url for file in files])\n",
    "\n",
    "def get_local_urls(vardic, ires='unused option'):\n",
    "  dom = vardic['domain']\n",
    "  rcm = vardic['rcm_name']\n",
    "  if var == 'tasmax':\n",
    "      return(sorted(\n",
    "        np.sort(glob.glob(f'/lustre/gmeteo/WORK/DATA/CORDEX-FPS-URB-RCC/{var}/{var}_{dom}_*_*-{rcm}_*'))\n",
    "        ))\n",
    "  elif var == 'tasmin':\n",
    "       return(sorted(\n",
    "        np.sort(glob.glob(f'/lustre/gmeteo/WORK/DATA/CORDEX-FPS-URB-RCC/{var}/*/*/{var}_{dom}_*_*-{rcm}_*'))\n",
    "        )) \n",
    "\n",
    "def dump_netcdf(var, location, opendap_urls, nc):\n",
    "  # Crop the area around the city and persist the data on a NetCDF file.\n",
    "  # We need to log in to ESGF to actually retrieve data.\n",
    "  #ESGF_login()\n",
    "  ds = xr.open_mfdataset(opendap_urls,\n",
    "    parallel=True, chunks={'time':100},\n",
    "    combine='nested', concat_dim='time',\n",
    "    drop_variables = ['time_bnds']\n",
    "  )\n",
    "  # Fix longitudes beyond 180\n",
    "  ds['lon'][:] = np.where(ds['lon'].values > 180, ds['lon'].values-360, ds['lon'].values)\n",
    "  #\n",
    "  #  Crop area around the city\n",
    "  #\n",
    "  dist = (ds['lon']-location[city]['lon'])**2 + (ds['lat']-location[city]['lat'])**2\n",
    "  [ilat], [ilon] = np.where(dist == np.min(dist))\n",
    "  ds_city = ds.isel(**{\n",
    "    select_name(rlat_names, ds.coords): slice(ilat-dlat,ilat+dlat),\n",
    "    select_name(rlon_names, ds.coords): slice(ilon-dlon,ilon+dlon)\n",
    "  })\n",
    "  ds_city.to_netcdf(nc, encoding = { var: {\"zlib\": True, \"complevel\": 9} })    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27f5bf-61c4-475a-a319-51693d666d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GHCNd stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a35e5-ad62-44ca-bf58-38fde706e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ghcnd_stations():\n",
    "  ghcnd_stations_url = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/doc/ghcnd-stations.txt'\n",
    "  ghcnd_stations_url = 'ghcnd-stations.txt'\n",
    "  ghcnd_stations_column_names = ['code', 'lat', 'lon', 'elev', 'name', 'net', 'numcode']\n",
    "  ghcnd_stations_column_widths = [   11,     9,    10,      7,     34,     4,       10 ]\n",
    "  df = pd.read_fwf(ghcnd_stations_url, header = 0,\n",
    "    widths = ghcnd_stations_column_widths,\n",
    "    names = ghcnd_stations_column_names\n",
    "  )\n",
    "  return(gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs = 'EPSG:4326'))\n",
    "\n",
    "def read_city_info():\n",
    "  df = pd.read_csv('city_info_out.csv', comment='#',\n",
    "    dtype = dict(domain = 'category', ktype = 'category')\n",
    "  )\n",
    "  return(gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs = 'EPSG:4326'))\n",
    "\n",
    "def nearby_stations(city, maxdis = 0.5, custom_city_center = False):\n",
    "  if custom_city_center:\n",
    "    # Use custom city center as defined above ('location' dict) instead of\n",
    "    # the centroid used in the FPS-URB-RCC database\n",
    "    rval = ghcnd_stations.assign(\n",
    "      dist = ghcnd_stations.distance(Point(\n",
    "        location[city]['lon'], location[city]['lat']\n",
    "      ))\n",
    "    )\n",
    "  else:\n",
    "    citydf = city_info.query(f'city == \"{city}\"').squeeze()\n",
    "    rval = ghcnd_stations.assign(dist = ghcnd_stations.distance(citydf.geometry))\n",
    "  rval.sort_values(by = 'dist', inplace = True)\n",
    "  rval = rval[rval.dist < maxdis].to_crs(epsg=3857)\n",
    "  return(rval)\n",
    "\n",
    "def get_ghcnd_df(code):\n",
    "    baseurl = 'http://meteo.unican.es/work/chus/ghcnd/data'\n",
    "    try:\n",
    "      rval = pd.read_csv(f'{baseurl}/{code[0]}/{code}.csv.gz',\n",
    "        compression = 'gzip',\n",
    "        index_col = 'DATE',\n",
    "        parse_dates = True,\n",
    "        low_memory = False # Avoid warning on mixed dtypes for some (unused) columns\n",
    "      )\n",
    "    except:\n",
    "      print(f'Problem downloading {code}')\n",
    "      rval = pd.DataFrame()\n",
    "    return(rval)\n",
    "\n",
    "def available_vars(station):\n",
    "  return(set(station.columns).intersection({'PRCP', 'TAVG', 'TMAX', 'TMIN', 'SNWD'}))\n",
    "\n",
    "def get_valid_timeseries(city, stations, var = 'PRCP', valid_threshold=0.8, idate='1979-01-01', fdate='2014-12-31'):\n",
    "  period = slice(idate, fdate)\n",
    "  ndays = (pd.to_datetime(fdate)-pd.to_datetime(idate)).days\n",
    "  valid_codes = []\n",
    "  valid_time_series = []\n",
    "  for stn_code in stations.code:\n",
    "    stn_data = get_ghcnd_df(stn_code)\n",
    "    if stn_data.empty:\n",
    "      continue\n",
    "    availvars = available_vars(stn_data)\n",
    "    if var in availvars:\n",
    "      valid_records = stn_data[var].loc[period].notna().sum()/ndays\n",
    "      if valid_records > valid_threshold:\n",
    "        print(f'{city} -- {stn_data.NAME[0]} - {var} has {100*valid_records:.1f}% valid records in {idate} to {fdate}')\n",
    "        valid_codes.append(stn_code)\n",
    "        valid_time_series.append(stn_data[var].loc[period])\n",
    "  return(stations[stations.code.isin(valid_codes)], valid_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e5fc8-4e54-4ba3-8749-8c8ec569fb2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdef9f-a5b9-4661-bb69-eda7dda3bc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_projected(index, dset, dest):\n",
    "  lonlat = ccrs.PlateCarree()\n",
    "  projvar = select_name(rotated_pole_names, dset.data_vars)\n",
    "  if projvar.startswith('rotated'):\n",
    "    proj = ccrs.RotatedPole(\n",
    "      pole_longitude=dset[projvar].grid_north_pole_longitude,\n",
    "      pole_latitude=dset[projvar].grid_north_pole_latitude\n",
    "    )\n",
    "  elif projvar.startswith('oblique'):\n",
    "    # To be implemented soon... (https://github.com/SciTools/cartopy/pull/2096)\n",
    "    proj = ccrs.ObliqueMercator(\n",
    "      pole_longitude=dset[projvar].grid_north_pole_longitude,\n",
    "      pole_latitude=dset[projvar].grid_north_pole_latitude\n",
    "    )\n",
    "  else:\n",
    "    try:\n",
    "        proj = ccrs.LambertConformal(\n",
    "        central_longitude=dset[projvar].longitude_of_central_meridian,\n",
    "        central_latitude=dset[projvar].latitude_of_projection_origin,\n",
    "        secant_latitudes=None,\n",
    "        standard_parallels=dset[projvar].standard_parallel\n",
    "      )\n",
    "    except:\n",
    "        # Dirty stuff to make oblique mercator projection \"work\"\n",
    "        # It is WIP in cartopy https://github.com/SciTools/cartopy/pull/2096\n",
    "        p4dict = dict([tuple(item[1:].split('=')) for item in dset[projvar].proj4_params.split(' ') if '=' in item])\n",
    "        globe = ccrs.Globe(datum='WGS84',\n",
    "          ellipse = p4dict.pop('ellps'), \n",
    "          semimajor_axis = p4dict.pop('a'), \n",
    "          semiminor_axis = p4dict.pop('b')\n",
    "        )\n",
    "        print(p4dict)\n",
    "        if not 'lonc' in p4dict: # RegCM on WAS-22\n",
    "          p4dict['lonc'] = p4dict['lon_0']\n",
    "        if not 'lat_0' in p4dict:\n",
    "          p4dict['lat_0'] = 0.\n",
    "        #proj = ccrs.Projection(p4dict, globe)\n",
    "        proj = ccrs.Mercator(central_longitude=float(p4dict.pop('lonc')), \n",
    "                            min_latitude=-89.99999, \n",
    "                            #max_latitude=float(p4dict.pop('alpha')), \n",
    "                            max_latitude=89.99999, \n",
    "                            globe=globe, \n",
    "                            latitude_true_scale=float(p4dict.pop('lat_0')), \n",
    "                            false_easting=float(p4dict.pop('x_0')), \n",
    "                            false_northing=float(p4dict.pop('y_0')), \n",
    "                            scale_factor=None)\n",
    "        print(f'Using custom projection from proj4_params string {proj}')\n",
    "  f = plt.figure(figsize=(12,12))\n",
    "  ax = plt.axes(projection=proj)\n",
    "  index.plot.pcolormesh(\n",
    "    #ax=ax, x='lon', y='lat', transform=lonlat, cmap=plt.cm.YlOrBr#, vmax=10\n",
    "    ax=ax, x='lon', y='lat', transform=lonlat, cmap=plt.cm.RdBu_r, vmax=vmax, vmin=vmin\n",
    "  )\n",
    "  plt.title(dset.model_id)\n",
    "  ax.coastlines(resolution='10m', linewidth=1, color='gray')\n",
    "  ax.scatter(location[city]['lon'], location[city]['lat'], transform=lonlat, s=200, facecolor='none', edgecolor='red')\n",
    "\n",
    "  ucdb_city.plot(ax=ax, transform=lonlat, facecolor=\"none\", edgecolor=\"red\")\n",
    "\n",
    "  st_uhin.plot(column='UHIN', ax=ax, transform=lonlat, edgecolor='k', cmap=plt.cm.RdBu_r, vmin=vmin, vmax=vmax, s=150)\n",
    "  f.savefig(dest + fname.replace('nc','png'), facecolor='white')\n",
    "  plt.show()\n",
    "  plt.close('all')\n",
    "  return(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481581b-c9ae-4e60-8f21-3b3c3eded43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projected(index, dset, vmin, vmax, dest):\n",
    "    lonlat = ccrs.PlateCarree() \n",
    "    proj = lonlat                  ####\n",
    "    f = plt.figure(figsize=(12,9))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    index.plot.pcolormesh(\n",
    "    ax=ax, x='lon', y='lat', transform=lonlat, cmap=plt.cm.RdBu_r, vmax=vmax, vmin=vmin\n",
    "    )\n",
    "    plt.title(dset.model_id)\n",
    "    ax.coastlines(resolution='10m', linewidth=1, color='gray')\n",
    "    ax.scatter(location[city]['lon'], location[city]['lat'], transform=lonlat, s=200, facecolor='none', edgecolor='red')\n",
    "    ucdb_city.plot(ax=ax, transform=lonlat, facecolor=\"none\", edgecolor=\"red\")\n",
    "    st_uhin.plot(column='UHIN', ax=ax, transform=lonlat, edgecolor='k', cmap=plt.cm.RdBu_r, vmin=vmin, vmax=vmax, s=150)\n",
    "    f.savefig(dest + fname.replace('nc','png'), facecolor='white')\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    return(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4d6f7-b834-4465-b027-09aa75baee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RCM_annual_cycle(ds_city, vmin, vmax, dest):\n",
    "    ds = ds_city\n",
    "    rlon_name = select_name(rlon_names, ds.coords)\n",
    "    rlat_name = select_name(rlat_names, ds.coords)\n",
    "    inner = ds[var].isel(**{rlon_name: dlon, rlat_name: dlat})\n",
    "    plt.figure()\n",
    "    (inner-inner.values).groupby('time.month').mean().plot()\n",
    "    for latshift in (-1,-2,-3):\n",
    "      outer = ds[var].isel(**{rlon_name: dlon, rlat_name: dlat+latshift}) - inner.values\n",
    "      #outer.groupby('time.month').mean().plot(ylim=(-4,1))\n",
    "      outer.groupby('time.month').mean().plot(ylim=(vmin,vmax))\n",
    "    plt.savefig(dest + '/' + var + '_RCM_annual_cycle_'+ ds.model_id + '_' + city + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd7120-3b15-45d5-803d-d7a535ae1b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Indices to identify urban areas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e58f-f64a-4dd9-80da-38eab9d7ca9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_argmax(ds):\n",
    "  ''' Day with the maximum tasmin on the city\n",
    "  \n",
    "  Shows spatial anomalies with respect to this maximum tasmin\n",
    "  '''\n",
    "  rlon_name = select_name(rlon_names, ds.coords)\n",
    "  rlat_name = select_name(rlat_names, ds.coords)\n",
    "  rval = (\n",
    "    ds[var]\n",
    "      .isel(time = (\n",
    "        ds[var]\n",
    "           .isel(**{rlon_name: dlon, rlat_name: dlat})\n",
    "           .argmax(dim='time')\n",
    "           .values\n",
    "        )\n",
    "      )\n",
    "  )\n",
    "  rval = rval - rval.isel(**{rlon_name: dlon, rlat_name: dlat}).values\n",
    "  return(rval)\n",
    "\n",
    "def index_average_n_highest(ds, n = 10):\n",
    "  ''' Average of n days with the largest tasmin on the city\n",
    "  \n",
    "  Averages spatial anomalies with respect to this maximum tasmin\n",
    "  '''\n",
    "  rlon_name = select_name(rlon_names, ds.coords)\n",
    "  rlat_name = select_name(rlat_names, ds.coords)\n",
    "  rval = (ds[var].isel(time = (ds[var]\n",
    "    .isel(**{rlon_name: dlon, rlat_name: dlat})\n",
    "    .argsort()[::-1][:n] # take the n highest\n",
    "    .values\n",
    "  )))\n",
    "  rval = rval - rval.isel(**{rlon_name: dlon, rlat_name: dlat}).values[:,None,None]\n",
    "  rval = rval.mean(dim='time')\n",
    "  return(rval)\n",
    "\n",
    "def index_average_spatial_anomaly(ds):\n",
    "  ''' Average of the grid point anomaly w.r.t. city center gridpoint\n",
    "  '''\n",
    "  rlon_name = select_name(rlon_names, ds.coords)\n",
    "  rlat_name = select_name(rlat_names, ds.coords)\n",
    "  rval = ds[var] - ds[var].isel(**{rlon_name: dlon, rlat_name: dlat}).values[:,None,None]\n",
    "  rval = rval.mean(dim='time')\n",
    "  return(rval)\n",
    "\n",
    "def index_quantile(ds):\n",
    "  '''Quantile 0.95 of the anomalies w.r.t the spatial average\n",
    "  '''\n",
    "  return(\n",
    "    (ds - ds.mean(dim=['rlon','rlat']))[var]\n",
    "      .groupby('time.season')\n",
    "      .quantile(0.95, 'time')\n",
    "      .sel(season='JJA')\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a0320-597d-4395-acc2-341578d2f765",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdc319-bfb8-462b-bccb-f958f2255406",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {\n",
    "     'Mexico City' : dict(lon=-99.0833, lat=19.4667, domain = 'CAM-22', #1\n",
    "                          vmin= -5, vmax = 5, valid_t = 0.8, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Buenos Aires' : dict(lon=-58.416, lat=-34.559, domain = 'SAM-22', #2\n",
    "                           vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'New York' : dict(lon=-74.2261, lat=40.8858, domain = 'NAM-22',#3\n",
    "                       vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Sydney' : dict(lon=151.01810, lat=-33.79170, domain = 'AUS-22', #4\n",
    "                     vmin= -3, vmax = 3, valid_t = 0.6, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Beijing' : dict(lon=116.41, lat=39.90, domain = 'EAS-22', #5\n",
    "                      vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Tokyo' : dict(lon = 139.84, lat = 35.65, domain = 'EAS-22', #6\n",
    "                    vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Jakarta' : dict(lon = 106.81, lat = -6.2, domain = 'SEA-22', #7\n",
    "                      vmin= -3, vmax = 3, valid_t = 0.35, maxdis = 1, vtmin = -10, vtmax = 10), \n",
    "     'Johannesburg' : dict(lon=28.183, lat=-25.733, domain = 'AFR-22', #8\n",
    "                           vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10), # Pretoria center station\n",
    "     'Riyadh' : dict(lon=46.73300, lat=24.7000, domain = 'WAS-22', #9\n",
    "                     vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10), #Urban frac\n",
    "     'Berlin' : dict(lon=13.4039, lat=52.4683, domain = 'EUR-11', #10\n",
    "                     vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 1, vtmin = -10, vtmax = 10),\n",
    "     'Paris' : dict(lon=  2.35, lat=48.85, domain = 'EUR-11',  #11\n",
    "                    vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 2, vtmin = -10, vtmax = 10), # Problems with the city (ucdb_city)\n",
    "     'London' : dict(lon= -0.13, lat=51.50, domain = 'EUR-11', #12\n",
    "                     vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 1, vtmin = -10, vtmax = 10),\n",
    "     'Madrid' : dict(lon= -3.70, lat=40.42, domain = 'EUR-11', #13\n",
    "                     vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "     'Los Angeles': dict(lon = -118.24, lat = 34.05, domain = 'NAM-22', #14\n",
    "                         vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 1, vtmin = -10, vtmax = 10),\n",
    "     'Montreal': dict(lon = -73.56, lat = 45.50, domain = 'NAM-22', #15\n",
    "                      vmin= -3, vmax = 3, valid_t = 0.5, maxdis = 0.5, vtmin = -10, vtmax = 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6ba8f-3f32-4864-87b4-a14f3f0f283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'tasmax': 'TMAX',\n",
    "             'tasmin': 'TMIN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a04c1-8fcd-4ac5-ac7f-9e7f3861845a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in variables.keys():\n",
    "    for city in location:\n",
    "        \n",
    "        dest_picture = dest + 'pictures/'\n",
    "        dest_data = dest + 'data/'\n",
    "        os.makedirs(dest_picture,  exist_ok=True)\n",
    "        os.makedirs(dest_data,  exist_ok=True)\n",
    "    \n",
    "        dlon = dlat = int(10 / (int(location[city]['domain'].split('-')[1])/11))\n",
    "        ucdb_city = ucdb_info.query(f'UC_NM_MN ==\"{city}\"').to_crs(crs = 'EPSG:4326')\n",
    "        if city == 'London':\n",
    "            ucdb_city = ucdb_city[ucdb_city['CTR_MN_NM'] == 'United Kingdom']\n",
    "        \n",
    "        compute_index = index_average_spatial_anomaly\n",
    "        \n",
    "        #stations\n",
    "        ghcnd_stations = read_ghcnd_stations()\n",
    "        city_info = read_city_info()\n",
    "        nearstat = nearby_stations(city, maxdis = location[city]['maxdis'], custom_city_center = True)\n",
    "        valid_stations, time_series = get_valid_timeseries(city, nearstat, variables[var], \n",
    "                                                           valid_threshold=location[city]['valid_t'])\n",
    "    \n",
    "        df = pd.concat(time_series, axis=1)/10. # convert to degrees\n",
    "        df = df.replace(99.9, np.nan)\n",
    "        df.columns = valid_stations.name\n",
    "    \n",
    "        for c in df.columns:\n",
    "          plt.figure()\n",
    "          df[c].plot(figsize=(14,3), title = c, xlim=('1979-01-01','2014-12-31'))\n",
    "    \n",
    "        filter_months = '' #(12,1,2)\n",
    "        if filter_months:\n",
    "          tfilter = df.index.month.isin(filter_months)\n",
    "          df = df[tfilter]\n",
    "    \n",
    "        anom = df.sub(df.iloc[:,0], axis='index')\n",
    "        anom = anom[~anom.isnull().any(axis=1)]\n",
    "        plt.figure()\n",
    "        p = anom.plot(figsize=(12,4)).legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "        plt.savefig(dest_picture + '/' + var + '_Time_Series_' + city + '.png', bbox_inches='tight')\n",
    "        plt.figure()\n",
    "        p = anom.groupby(anom.index.month).mean().plot(ylim=(-10,10)).legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "        plt.savefig(dest_picture + '/' + var + '_Gauges_annual_cycle_' + city + '.png', bbox_inches='tight')\n",
    "    \n",
    "        st_uhin = valid_stations.set_index('name').to_crs(crs = 'EPSG:4326').assign(UHIN = anom.mean())\n",
    "    \n",
    "        for rcm in rcms[location[city]['domain']]:\n",
    "          print(f'Reading data for {rcm} ...')\n",
    "          #opendap_urls = get_opendap_urls(search_dic(var, location[city]['domain'], rcm))\n",
    "          opendap_urls = get_local_urls(search_dic(var, location[city]['domain'], rcm))\n",
    "          print(opendap_urls)\n",
    "          fname = os.path.basename(opendap_urls[0])[:-21] + f'_{city.replace(\" \",\"\")}.nc'\n",
    "          if not os.path.exists(dest_data + fname):\n",
    "            dump_netcdf(var, location, opendap_urls, dest_data + fname)\n",
    "          ds_city = xr.open_dataset(dest_data + fname)\n",
    "          valid_times = ds_city.indexes['time'].normalize().isin(anom.index)\n",
    "          print('Number of days with data: ' + str(len(valid_times)))\n",
    "          if len(valid_times) < 365*10:\n",
    "              sys.exit()\n",
    "          index = compute_index(ds_city.sel({'time': valid_times}))\n",
    "          ax = plot_projected(index, ds_city, location[city]['vmin'], location[city]['vmax'], dest_picture)\n",
    "          ## RCM annual cycle\n",
    "          plot_RCM_annual_cycle(ds_city, location[city]['vtmin'], location[city]['vtmax'], dest_picture)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
